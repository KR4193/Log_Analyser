{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Package Installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install sentence-transformers\n",
    "%pip install langchain_community\n",
    "%pip install langchain_community\n",
    "%pip install openai\n",
    "%pip install faiss-cpu\n",
    "%pip install langchain_openai\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import AzureOpenAI\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "client = AzureOpenAI(\n",
    "  azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\"), \n",
    "  api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),  \n",
    "  api_version=\"2024-02-01\"\n",
    ")\n",
    "assert client is not None, \"Failed to create AzureOpenAI client\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File and Database Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_file_path = \"C:/Users/kr4193/Desktop/Log_error_reporter/Prep_work/clean_Geiger_for_LLMs.log\"\n",
    "dirty_log_file = \"C:/Users/kr4193/Desktop/Log_error_reporter/Prep_work/SMOKE-ZSB-DP12-002.log\"\n",
    "embedding_model = \"all-MiniLM-L6-v2\"\n",
    "database_name = \"sample_db\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Chunks and splitting the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_chunks(filename):\n",
    "    data = []\n",
    "    final_chunk = []\n",
    "\n",
    "    loader = TextLoader(filename, encoding=\"utf-8\")\n",
    "    rawdata = loader.load()\n",
    "    if rawdata:\n",
    "        text_splitter = CharacterTextSplitter(chunk_size=1500, chunk_overlap=50)\n",
    "        data = text_splitter.split_documents(rawdata)\n",
    "        if data:\n",
    "            print(f\"generate_chunks - {filename}\")\n",
    "            final_chunk += data\n",
    "        else:\n",
    "            print(f\"generate_chunks - data is None\")\n",
    "    else:\n",
    "        print(f\"generate_chunks - rawdata is None\")\n",
    "\n",
    "    print(f\"{filename} data chunks ready for embedding\")\n",
    "\n",
    "        # Add more conditions for other file types if needed\n",
    "    print(\"prepare_data_chunks: finished\")\n",
    "    return final_chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and Load the Vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vectordb(filepath, databasename):\n",
    "    data = generate_chunks(filepath)\n",
    "    if data:\n",
    "        print(f\"Starting to create {filepath} ...\")\n",
    "        embeddings = HuggingFaceEmbeddings(model_name=embedding_model)\n",
    "        print(f\"embedding : {embeddings}\")\n",
    "        if embeddings:\n",
    "            vdatabase = FAISS.from_documents(data, embeddings)\n",
    "            vdatabase.save_local(databasename)\n",
    "            print(f\" vectordatabase {databasename} ready...\")\n",
    "        else:\n",
    "            print(f\"Empty Embeddings\")\n",
    "        return vdatabase\n",
    "    else:\n",
    "        print(\"chunk data received is null, exiting database creation\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vector_db(embedding_model_name, vector_db_name):\n",
    "    print(f\" Loading {vector_db_name} ...\")\n",
    "    if os.path.exists(vector_db_name):\n",
    "        print(f\" database {vector_db_name} present!!\")\n",
    "        embeddings = HuggingFaceEmbeddings(model_name=embedding_model_name)\n",
    "        if embeddings:\n",
    "            vector_db = FAISS.load_local(vector_db_name, embeddings, allow_dangerous_deserialization=True)\n",
    "            print(f\" Loading {vector_db_name} Done!!\")\n",
    "        else:\n",
    "            print(f\"Empty Embeddings\")\n",
    "    else:\n",
    "        print(f\" No file path found for {vector_db_name}..\")\n",
    "    return vector_db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=\"\"\"\n",
    "    You are a helpful assistant. Use the following context to answer the question.\n",
    "    Context: {context}\n",
    "    Question: {question}\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qa_bot():\n",
    "    vectordb = get_retriever()\n",
    "    retriever = vectordb.as_retriver(search_kwargs = {\"k\":10})\n",
    "    query = \"What is the error in the log file?\"\n",
    "    while query != \"quit\":\n",
    "        query = input(\"Enter your query: \")\n",
    "        output = chain.invoke(query)\n",
    "        print(output)\n",
    "    qa_bot()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 6698, which is longer than the specified 1500\n",
      "Created a chunk of size 4368, which is longer than the specified 1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generate_chunks - C:/Users/kr4193/Desktop/Log_error_reporter/Prep_work/clean_Geiger_for_LLMs.log\n",
      "C:/Users/kr4193/Desktop/Log_error_reporter/Prep_work/clean_Geiger_for_LLMs.log data chunks ready for embedding\n",
      "prepare_data_chunks: finished\n",
      "Starting to create C:/Users/kr4193/Desktop/Log_error_reporter/Prep_work/clean_Geiger_for_LLMs.log ...\n",
      "embedding : model_name='all-MiniLM-L6-v2' cache_folder=None model_kwargs={} encode_kwargs={} multi_process=False show_progress=False\n",
      " vectordatabase sample_db ready...\n",
      " Loading sample_db ...\n",
      " database sample_db present!!\n",
      " Loading sample_db Done!!\n",
      "retrieved_output: [Document(id='f7bcc214-e9b2-46a2-ace5-de49d381b181', metadata={'source': 'C:/Users/kr4193/Desktop/Log_error_reporter/Prep_work/clean_Geiger_for_LLMs.log'}, page_content='Suite SnmpCleanCutterTests: passed\\nPassed         : 2\\nFailed         : 0\\nSkipped        : 0\\nError          : 0\\nCanceled       : 0\\nExpected Fail  : 0'), Document(id='110eb360-3911-4fab-86bc-3528c6c26459', metadata={'source': 'C:/Users/kr4193/Desktop/Log_error_reporter/Prep_work/clean_Geiger_for_LLMs.log'}, page_content='Leaving working directory.\\nChdir back to \\'/home/swgroup/data/jenkins/workspace/LINK_OS_7_2_ATF_Geiger_Tests_ZT6X0_Cutter\\'\\nRemoving scratch folder.\\nExecute module: print.cutter\\nEntering working directory.\\nSource /home/swgroup/data/jenkins/workspace/LINK_OS_7_2_ATF_Geiger_Tests_ZT6X0_Cutter/atf/tests/print/cutter_files exists, copying files\\nChdir to \\'/home/swgroup/data/jenkins/workspace/LINK_OS_7_2_ATF_Geiger_Tests_ZT6X0_Cutter/output/scratch\\'\\nEntering suite: CutterDetectTests\\nExecute suite: CutterDetectTests\\nValidating suite...\\nValidation completed.\\nCalling suite setup...\\nAttempt #1 to connect to tcp_10.14.207.146:9100.\\nEventBroker: Publishing connection_made\\nConnected to tcp_10.14.207.146:9100.\\nConnected to interface \\'network\\' successfully.\\nConnection name \\'tcp_10.14.207.146:9100\\'\\nSetting up fixture (<class \\'atf.tools.sgd.SgdFixture\\'>)...\\nFixture (<class \\'atf.tools.sgd.SgdFixture\\'>) setup success!\\nRe-using existing \\'network\\' connection.\\nCollected: b\\'hybrid_xml_zpl\\'\\ngetvar b\\'device.languages\\' = b\\'hybrid_xml_zpl\\'\\nSaving SGD b\\'device.languages\\' value b\\'hybrid_xml_zpl\\'.\\nRe-using existing \\'network\\' connection.\\nCollected: b\\'no motion\\'\\ngetvar b\\'ezpl.head_close_action\\' = b\\'no motion\\'\\nSaving SGD b\\'ezpl.head_close_action\\' value b\\'no motion\\'.\\nRe-using existing \\'network\\' connection.\\nCollected: b\\'cutter\\'\\ngetvar b\\'ezpl.print_mode\\' = b\\'cutter\\'\\nSaving SGD b\\'ezpl.print_mode\\' value b\\'cutter\\'.\\nRe-using existing \\'network\\' connection.\\nCollected: b\\'N\\'\\ngetvar b\\'media.backfeed\\' = b\\'N\\'\\nSaving SGD b\\'media.backfeed\\' value b\\'N\\'.\\nRe-using existing \\'network\\' connection.\\nCollected: b\\'no motion\\'\\ngetvar b\\'ezpl.power_up_action\\' = b\\'no motion\\'\\nSaving SGD b\\'ezpl.power_up_action\\' value b\\'no motion\\'.\\nRe-using existing \\'network\\' connection.\\nCollected: b\\'continuous\\'\\ngetvar b\\'ezpl.media_type\\' = b\\'continuous\\'\\nSaving SGD b\\'ezpl.media_type\\' value b\\'continuous\\'.\\nRe-using existing \\'network\\' connection.\\nCollected: b\\'0\\'\\ngetvar b\\'media.tof\\' = b\\'0\\'\\nSaving SGD b\\'media.tof\\' value b\\'0\\'.\\nsetvar b\\'device.languages\\' = b\\'epl_zpl\\'\\nsetvar b\\'ezpl.head_close_action\\' = b\\'no motion\\'\\nCollected: b\\'300\\'\\ngetvar b\\'head.resolution.in_dpi\\' = b\\'300\\'\\nYielding cases for: CutterDetectTests_BasicTests\\nRunning test: detect_cutter_device_current_pcc\\nValidation completed.\\nCalling setup_common for fixture of type <class \\'atf.tools.sgd.SgdFixture\\'>.\\nCollected: b\\'ZT61043-T110100Z\\'\\ngetvar b\\'device.current_configuration_number\\' = b\\'ZT61043-T110100Z\\'\\nTest outcome   : passed\\nCalling teardown_common for fixture of type SgdFixture.\\nRunning test: detect_cutter_installed\\nValidation completed.\\nCalling setup_common for fixture of type <class \\'atf.tools.sgd.SgdFixture\\'>.\\nCollected: b\\'yes\\'\\ngetvar b\\'device.cutter_installed\\' = b\\'yes\\'\\nTest outcome   : passed\\nCalling teardown_common for fixture of type SgdFixture.\\nTearing down fixture of type SgdFixture.\\nRestoring 7 saved SGDs.\\nRe-using existing \\'network\\' connection.\\nsetvar b\\'device.languages\\' = b\\'hybrid_xml_zpl\\'\\nRe-using existing \\'network\\' connection.\\nsetvar b\\'ezpl.head_close_action\\' = b\\'no motion\\'\\nRe-using existing \\'network\\' connection.\\nsetvar b\\'ezpl.print_mode\\' = b\\'cutter\\'\\nRe-using existing \\'network\\' connection.\\nsetvar b\\'media.backfeed\\' = b\\'N\\'\\nRe-using existing \\'network\\' connection.\\nsetvar b\\'ezpl.power_up_action\\' = b\\'no motion\\'\\nRe-using existing \\'network\\' connection.\\nsetvar b\\'ezpl.media_type\\' = b\\'continuous\\'\\nRe-using existing \\'network\\' connection.\\nsetvar b\\'media.tof\\' = b\\'0\\'\\nDisconnected from tcp_10.14.207.146:9100.\\nEventBroker: Publishing connection_lost\\nWriting out suite report for CutterDetectTests\\nWriting with reporter type JUnitSuiteReporter\\nWriting with reporter type HTMLSuiteReporter\\nSuccessfuly imported extension module \"markdown.extensions.meta\".\\nSuccessfully loaded extension \"markdown.extensions.meta.MetaExtension\".\\nEntering suite: ErrorCutterTests\\nExecute suite: ErrorCutterTests\\nValidating suite...\\nNormal validation failure encountered via assertion exception.\\nYielding cases for: ErrorCutterTests_BasicTests\\nWriting out suite report for ErrorCutterTests\\nWriting with reporter type JUnitSuiteReporter\\nWriting with reporter type HTMLSuiteReporter\\nSuccessfuly imported extension module \"markdown.extensions.meta\".\\nSuccessfully loaded extension \"markdown.extensions.meta.MetaExtension\".\\nSuite CutterDetectTests: passed\\nPassed         : 2\\nFailed         : 0\\nSkipped        : 0\\nError          : 0\\nCanceled       : 0\\nExpected Fail  : 0')]\n",
      "vectordb ready\n"
     ]
    }
   ],
   "source": [
    "# Press the green button in the gutter to run the script.\n",
    "# if __name__ == '__main__':\n",
    "\n",
    "# Create vector data base\n",
    "local_vectordb = create_vectordb(log_file_path, database_name)\n",
    "\n",
    "# Load existing database using db name\n",
    "vector_db = load_vector_db(embedding_model, database_name)\n",
    "\n",
    "# Initialze the retriver with retrieval method\n",
    "#retriever = jira_vector_db.as_retriever(search_type=\"similarity_score_threshold\", search_kwargs={\"score_threshold\": 0.1})\n",
    "retriever = vector_db.as_retriever(search_type=\"mmr\", search_kwargs={\"k\":2})\n",
    "\n",
    "# Query the database to get symantical search output\n",
    "retrieved_output = retriever.invoke(\"What is this document about?\")\n",
    "print(f\"retrieved_output: {retrieved_output}\")\n",
    "print(f\"vectordb ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "client.invoke('tell me a joke')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prompt = ChatPromptTemplate.from_template(\"tell me a short joke\")\n",
    "chain = prompt  | client | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why so serious? All right, here’s a little joke for you:\n",
      "\n",
      "Why don’t criminals gamble in Gotham City?\n",
      "\n",
      "Because the Joker always has the last laugh!\n"
     ]
    }
   ],
   "source": [
    "question = \"Tell a joke\"\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o\", # model = \"deployment_name\".\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are the joker the DC villian from batman. You make only evil jokes\"},\n",
    "        {\"role\": \"user\", \"content\": \"tell me a joke\"},\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o\", # model = \"deployment_name\".\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"system\", \"content\": \"The fromat of tests is as follows Test suites -> Test modules -> Test cases , Test cases are the lowest level of granularity \"},\n",
    "        {\"role\": \"system\", \"content\": \"Test Suites are started at the line Entering suites: <suitename> and ended at Suite <suitename> <result>\"},\n",
    "        {\"role\": \"system\", \"content\": \"Test models are started at Execute module: <module_name>\"},\n",
    "        {\"role\": \"system\", \"content\": \"Test cases are started at the line Running test: <test_name> and ended at line <test_name>: <result>\"}, \n",
    "        {\"role\": \"system\", \"content\": \"Always Ignore all passed test cases and Suites\"},\n",
    "        {\"role\": \"system\", \"content\": \"Always Identify and Classify the Failed tests into one of the 3 categories - Product issues, ATF Script Issues & Setup issues\"},\n",
    "        {\"role\": \"system\", \"content\": \"Always display for failed test suites only in the follwing format {Suite, Module,\\\n",
    "          Synopsis of the test Failed, test name, Failure category, Reason for categorisation } \"},\n",
    "        {\"role\": \"user\", \"content\": f\"Here is the script that was used to run the test {content}\"},\n",
    "        {\"role\": \"user\", \"content\": f\"Here is a log entry for analysis:\\n{log_content}\\n\\nQuestion: {question}\\n\"},\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "with open(\"output_context.txt\", \"w\") as output_file:\n",
    "    output_file.write(response.choices[0].message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
